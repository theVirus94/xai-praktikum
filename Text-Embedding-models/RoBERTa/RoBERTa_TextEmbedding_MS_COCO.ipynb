{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80ad5bb8",
   "metadata": {},
   "source": [
    "### Language Model Used:\n",
    "\t - The RoBERTa model was proposed in RoBERTa: A Robustly Optimized BERT Pretraining Approach by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. It is based on Googleâ€™s BERT model released in 2018.\n",
    "\t - [Blog-Post](https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/)\n",
    "\t - [Research Paper](https://arxiv.org/pdf/1907.11692)\n",
    "     - [Documentation for python](https://huggingface.co/transformers/model_doc/roberta.html)\n",
    "\n",
    "\n",
    "### Hardware Requirements:\n",
    "\t - Python 3.6 and above\n",
    "\t - Pytorch, Transformers and All the stock Python ML Libraries\n",
    "\t - GPU enabled setup \n",
    "\n",
    "### Intro:\n",
    "In this Notebook,  I introduce how to get text embedding from RoBERTa (/BERT/ALBERT/etc.).  \n",
    "There are 2 methods to get text embedding from RoBERTa.\n",
    "1. get CLS Token\n",
    "2. pool RoBERTa output (RoBERTa output = word embeddings)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fadfc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install transformers==3.0.2\n",
    "#!conda install -c conda-forge transformers\n",
    "#!pip install fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6a351d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries needed\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torch import cuda\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import fiftyone as fo\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "273fc726",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Settings:\n",
    "    batch_size=16\n",
    "    max_len=350\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seed = 318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d12b6e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "set_seed(Settings.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-offer",
   "metadata": {
    "papermill": {
     "duration": 0.020969,
     "end_time": "2021-05-16T08:14:18.566807",
     "exception": false,
     "start_time": "2021-05-16T08:14:18.545838",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2804bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #super(RobertaClass, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
    "#         output = self.roberta(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        output = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a516d70e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaClass(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaClass()\n",
    "model.to(Settings.device) #has 768-output embbedings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-dodge",
   "metadata": {
    "papermill": {
     "duration": 0.020382,
     "end_time": "2021-05-16T08:14:18.469799",
     "exception": false,
     "start_time": "2021-05-16T08:14:18.449417",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset\n",
    "### Preparing the Dataset and Dataloader\n",
    "\n",
    "I will start with defining few key variables that will be used later during the training/fine tuning stage.\n",
    "Followed by creation of Dataset class - This defines how the text is pre-processed before sending it to the neural network. I will also define the Dataloader that will feed  the data in batches to the neural network for suitable training and processing. \n",
    "Dataset and Dataloader are constructs of the PyTorch library for defining and controlling the data pre-processing and its passage to neural network. For further reading into Dataset and Dataloader read the [docs at PyTorch](https://pytorch.org/docs/stable/data.html)\n",
    "\n",
    "-  I am using the Roberta tokenizer which uses the `encode_plus` method to perform tokenization and generate the necessary outputs, namely: `ids`, `attention_mask`\n",
    "- To read further into the tokenizer, [refer to this document](https://huggingface.co/transformers/model_doc/roberta.html#robertatokenizer)\n",
    "- `target` is the encoded category on the news headline. \n",
    "-  I am using the COCO APIs to load the COCO-Dataset\n",
    "\n",
    "#### Dataloader\n",
    "- Dataloader is used to for creating training and validation dataloader that load data to the neural network in a defined manner. This is needed because all the data from the dataset cannot be loaded to the memory at once, hence the amount of dataloaded to the memory and then passed to the neural network needs to be controlled.\n",
    "- This control is achieved using the parameters such as `batch_size` and `max_len`.\n",
    "- Training and Validation dataloaders are used in the training and validation part of the flow respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8eb909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eligible-google",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-16T08:14:18.516769Z",
     "iopub.status.busy": "2021-05-16T08:14:18.516034Z",
     "iopub.status.idle": "2021-05-16T08:14:18.523833Z",
     "shell.execute_reply": "2021-05-16T08:14:18.524385Z"
    },
    "papermill": {
     "duration": 0.032329,
     "end_time": "2021-05-16T08:14:18.524594",
     "exception": false,
     "start_time": "2021-05-16T08:14:18.492265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainValidDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.text = df[\"excerpt\"].values  #TODO: adjust to coco\n",
    "        self.target = df[\"target\"].values #TODO: adjust to coco\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        texts = self.text[idx] #text = str(self.text[index]),text = \" \".join(text.split()) #TODO: adjust to coco\n",
    "        tokenized = self.tokenizer.encode_plus(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        ids = tokenized[\"input_ids\"]\n",
    "        mask = tokenized[\"attention_mask\"]\n",
    "        #token_type_ids = tokenized[\"token_type_ids\"] #TODO: adjust to coco\n",
    "        targets = self.target[idx]\n",
    "        \n",
    "        return {\n",
    "            \"ids\": torch.LongTensor(ids),\n",
    "            \"mask\": torch.LongTensor(mask),\n",
    "            #'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long), #TODO: adjust to coco\n",
    "            \"targets\": torch.tensor(targets, dtype=torch.float32)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-ownership",
   "metadata": {
    "papermill": {
     "duration": 0.021293,
     "end_time": "2021-05-16T08:14:27.751412",
     "exception": false,
     "start_time": "2021-05-16T08:14:27.730119",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Get Text Embeddings\n",
    "\n",
    "### Get Input for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896e323d",
   "metadata": {},
   "source": [
    "Untill we can load the COCO Dataset (Check at the bottom), I will stub the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87bfe707",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ids=[] #some sentence\n",
    "mask = []\n",
    "token_type_ids= []\n",
    "\n",
    "def testing_tokenizer(text):\n",
    "        tokenized = tokenizer.encode_plus(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=Settings.max_len,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        ids = tokenized[\"input_ids\"]\n",
    "        mask = tokenized[\"attention_mask\"]\n",
    "        #token_type_ids = tokenized[\"token_type_ids\"] #TODO: adjust to coco\n",
    "        \n",
    "        return {\n",
    "            \"ids\": torch.LongTensor(ids),\n",
    "            \"mask\": torch.LongTensor(mask),\n",
    "            #'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long), #TODO: adjust to coco\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "31ff336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts=[\"Saw met applauded favourite deficient engrossed concealed and her\",\"A series of escapades demonstrating the adage\"]    \n",
    "text=\"Saw met applauded favourite deficient engrossed concealed and her\"\n",
    "tokenizer_output = testing_tokenizer(text)\n",
    "\n",
    "# print(tokenizer_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "83027b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 350])\n",
      "torch.Size([1, 350])\n",
      "1= num of texts, 350 = num of word tokens per a text  \n"
     ]
    }
   ],
   "source": [
    "ids =tokenizer_output[\"ids\"].to(Settings.device).unsqueeze(0)\n",
    "mask=tokenizer_output[\"mask\"].to(Settings.device).unsqueeze(0)\n",
    "# target=tokenizer_output[\"target\"].to(Settings.device).unsqueeze(0)\n",
    "\n",
    "print(ids.shape)\n",
    "print(mask.shape)\n",
    "# print(targets.shape)\n",
    "print(\"{}= num of texts, {} = num of word tokens per a text  \".format(ids.shape[0],ids.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-highlight",
   "metadata": {
    "papermill": {
     "duration": 0.022699,
     "end_time": "2021-05-16T08:14:28.801581",
     "exception": false,
     "start_time": "2021-05-16T08:14:28.778882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "350 = num of word tokens per a text  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-degree",
   "metadata": {
    "papermill": {
     "duration": 0.025272,
     "end_time": "2021-05-16T08:14:44.002185",
     "exception": false,
     "start_time": "2021-05-16T08:14:43.976913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "meaning of pooler output will be explained later.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e3967e",
   "metadata": {
    "papermill": {
     "duration": 0.021293,
     "end_time": "2021-05-16T08:14:27.751412",
     "exception": false,
     "start_time": "2021-05-16T08:14:27.730119",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f7e5077",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0567,  0.0669, -0.0062,  ..., -0.1220, -0.0256, -0.0132],\n",
       "         [ 0.0362,  0.1375, -0.1234,  ..., -0.0722,  0.0101,  0.2137],\n",
       "         [ 0.0526,  0.1547, -0.2172,  ..., -0.2524, -0.0212,  0.1244],\n",
       "         ...,\n",
       "         [ 0.0210,  0.0784, -0.1106,  ..., -0.3599,  0.0994,  0.0343],\n",
       "         [ 0.0210,  0.0784, -0.1106,  ..., -0.3599,  0.0994,  0.0343],\n",
       "         [ 0.0210,  0.0784, -0.1106,  ..., -0.3599,  0.0994,  0.0343]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-1.2758e-02, -2.1751e-01, -2.0590e-01, -8.2266e-02,  1.1925e-01,\n",
       "          1.8918e-01,  2.5004e-01, -9.0720e-02, -5.8725e-02, -1.7462e-01,\n",
       "          2.3513e-01, -7.6291e-03, -8.3253e-02,  7.6222e-02, -1.3445e-01,\n",
       "          4.9221e-01,  2.2376e-01, -4.4706e-01,  2.0791e-02, -2.1818e-02,\n",
       "         -2.5792e-01,  7.0410e-02,  4.5894e-01,  3.0744e-01,  1.5259e-01,\n",
       "          6.5493e-02, -1.3735e-01, -1.5289e-03,  1.7140e-01,  2.2063e-01,\n",
       "          2.7516e-01,  5.4530e-02,  1.0123e-01,  2.3078e-01, -2.3053e-01,\n",
       "          5.6373e-02, -3.2364e-01,  3.4834e-02,  2.5585e-01, -1.7025e-01,\n",
       "         -6.0510e-02,  1.4271e-01,  2.1398e-01, -1.4402e-01, -1.0757e-01,\n",
       "          3.9238e-01,  2.3035e-01,  5.1897e-02, -1.2434e-01, -7.1027e-02,\n",
       "         -3.6801e-01,  3.3535e-01,  2.8548e-01,  1.9109e-01,  1.1495e-02,\n",
       "          3.7696e-02, -1.4123e-01,  2.5971e-01, -9.1335e-02, -9.6119e-02,\n",
       "         -1.1984e-01, -1.7321e-01, -2.7712e-02, -5.9135e-02,  2.4032e-02,\n",
       "         -1.1390e-01,  6.9897e-02, -1.4020e-01, -1.4536e-01,  6.9314e-02,\n",
       "         -6.8496e-02,  1.3404e-01,  1.4515e-01, -2.6799e-01, -2.6900e-01,\n",
       "          5.0115e-02, -5.9003e-01, -1.1819e-01,  2.9677e-01,  4.1570e-01,\n",
       "         -1.2574e-01,  1.8848e-01,  1.5782e-02,  1.9034e-01, -4.4837e-02,\n",
       "         -5.9325e-02, -4.6237e-02, -9.4577e-02,  2.0286e-01,  2.5491e-01,\n",
       "         -1.8377e-01, -3.6984e-01,  5.7323e-02,  2.1876e-02, -9.5286e-02,\n",
       "          1.3581e-02, -1.6507e-02, -9.1672e-02, -1.5024e-01, -1.7788e-01,\n",
       "          3.3745e-02, -2.7893e-01, -1.0977e-01,  2.6989e-01, -4.7529e-02,\n",
       "         -2.0964e-01, -4.1698e-03,  3.1752e-01,  7.8556e-02, -1.4330e-01,\n",
       "         -1.6577e-01,  4.1836e-01,  2.8753e-01,  4.2813e-02,  2.7234e-02,\n",
       "          1.7856e-01,  1.2239e-01, -2.8463e-01,  4.2142e-01, -3.0772e-01,\n",
       "         -1.8275e-02, -9.5680e-02,  1.1233e-01,  1.4089e-01, -2.1447e-01,\n",
       "          2.6027e-01,  1.3349e-01,  2.5443e-01,  1.7639e-01,  7.9226e-02,\n",
       "         -3.2983e-02,  1.3630e-01, -1.0188e-01,  1.3261e-01,  2.1110e-01,\n",
       "          1.1797e-01,  1.1941e-02, -3.1025e-01, -2.1585e-01,  2.4442e-01,\n",
       "          3.3794e-01,  1.5693e-01, -3.2266e-02,  2.0902e-01,  8.2592e-02,\n",
       "          2.1481e-01,  1.2592e-01, -3.9443e-01,  4.0396e-02,  3.2392e-01,\n",
       "          1.0720e-01,  1.6291e-01, -6.1713e-02, -2.9278e-01, -2.4997e-01,\n",
       "         -1.0072e-01,  3.1196e-02, -3.1913e-01, -1.1743e-01,  3.4879e-01,\n",
       "         -3.2583e-03,  1.5447e-02, -1.3103e-01, -2.1016e-01, -4.8104e-02,\n",
       "         -1.1197e-01,  2.3297e-02,  9.8203e-02, -7.6561e-02, -4.2319e-01,\n",
       "         -8.2615e-02, -5.2848e-01, -1.1709e-01,  2.0524e-01, -2.9941e-01,\n",
       "          2.2378e-01, -2.8417e-01,  5.8595e-02,  3.8281e-01,  2.6216e-02,\n",
       "         -1.5454e-02, -1.7589e-01, -1.9929e-02,  9.8964e-02,  3.0967e-01,\n",
       "          2.4761e-01, -4.0005e-01,  7.0972e-02,  1.4480e-01,  2.5545e-01,\n",
       "          1.6260e-01, -3.8940e-02, -1.3141e-01,  1.1586e-01, -2.0717e-01,\n",
       "          1.7538e-01, -2.3576e-01,  1.7870e-01, -2.4563e-01, -1.9226e-01,\n",
       "          2.6600e-01, -3.7021e-01, -3.8405e-02,  8.5288e-02,  2.5926e-01,\n",
       "          1.3831e-02, -3.1491e-02, -8.1882e-02,  9.5704e-02,  1.8432e-01,\n",
       "          1.3163e-01, -3.8459e-01,  2.5508e-01, -1.2012e-02, -2.6821e-02,\n",
       "         -4.1289e-02,  1.6012e-01,  2.4323e-01,  1.1025e-01, -3.8775e-01,\n",
       "         -1.2824e-01,  1.0882e-01,  2.8745e-01, -2.2845e-01,  1.5544e-01,\n",
       "         -2.7642e-01, -3.5277e-01, -1.4536e-01,  2.1249e-01,  2.2357e-01,\n",
       "          1.5805e-01, -2.7498e-01,  1.7070e-01, -8.2384e-02, -4.2252e-01,\n",
       "         -3.5541e-01, -9.1322e-02,  2.3966e-01,  1.7420e-01,  1.9668e-01,\n",
       "          2.4918e-01,  3.9273e-02,  1.1736e-01,  1.4425e-01,  1.5200e-01,\n",
       "         -1.3720e-01,  1.6767e-01, -3.5732e-01, -6.2774e-02, -2.3865e-01,\n",
       "         -1.8708e-01, -1.9581e-01,  3.7625e-01, -2.2508e-01,  2.3733e-01,\n",
       "          3.8016e-01, -2.8823e-01, -1.1762e-01,  1.5109e-01,  8.9404e-02,\n",
       "          9.7238e-02, -1.5270e-01,  2.0793e-01,  1.5771e-01, -1.0077e-01,\n",
       "          2.1971e-01,  7.5406e-03,  2.4950e-01,  1.6364e-01,  1.0963e-01,\n",
       "          1.5589e-01,  1.1456e-01, -1.6498e-01,  4.1548e-02,  2.2644e-02,\n",
       "         -3.1976e-02, -2.4229e-01, -1.6474e-01,  2.2153e-01, -3.0891e-02,\n",
       "          2.7038e-02, -1.7286e-01, -9.8176e-02,  1.7146e-02,  4.0285e-01,\n",
       "         -3.6591e-01,  2.4467e-01,  7.9542e-02,  1.5446e-01, -2.4581e-01,\n",
       "         -2.1680e-01,  1.0129e-01,  1.4446e-01, -3.7306e-01,  2.8623e-03,\n",
       "          1.5159e-01,  1.0497e-01,  1.9843e-01,  2.5802e-01, -3.9563e-03,\n",
       "         -7.4753e-02,  4.7453e-01, -1.6554e-01, -1.2879e-01,  2.4869e-01,\n",
       "         -2.6785e-01, -2.8691e-01,  2.3078e-01, -2.0748e-02,  3.0385e-01,\n",
       "          1.1723e-01,  4.8496e-02,  8.7224e-02, -5.9654e-01,  7.5396e-02,\n",
       "         -4.4413e-01, -2.1217e-02,  1.5698e-02, -8.4866e-02, -2.0285e-01,\n",
       "          1.3149e-01,  2.9373e-01, -2.4732e-01, -4.0844e-02,  2.0223e-01,\n",
       "          6.3612e-02, -1.3812e-01,  4.5632e-01, -8.3634e-03,  1.8072e-01,\n",
       "         -9.7758e-02,  2.5232e-01, -2.0367e-01,  2.6008e-01, -2.6376e-01,\n",
       "         -8.9588e-02,  1.4226e-02,  8.4296e-02,  5.0926e-02, -4.8051e-02,\n",
       "         -3.4655e-01,  2.2539e-01, -5.0147e-03, -6.4253e-02, -3.8664e-02,\n",
       "          8.7648e-02, -1.7801e-02,  3.8257e-02,  6.2182e-02,  3.2623e-01,\n",
       "          2.3162e-01, -7.8510e-03, -3.4505e-01,  2.6293e-03, -1.1063e-01,\n",
       "          3.5688e-02,  1.0105e-02, -1.9641e-02,  4.2467e-01, -8.9328e-02,\n",
       "         -3.8252e-03, -1.5351e-01,  2.5278e-01,  1.7396e-01,  1.0177e-01,\n",
       "          1.1429e-01,  5.2889e-02,  1.2899e-01, -6.8499e-02,  1.5534e-04,\n",
       "         -1.5050e-01, -2.2276e-01, -3.0146e-01,  2.1398e-01, -2.4755e-01,\n",
       "         -1.7213e-01,  1.3839e-01,  1.9229e-01, -1.3863e-01,  1.2813e-01,\n",
       "          2.9926e-01,  9.1929e-02, -1.4779e-01,  2.5472e-01, -1.2323e-01,\n",
       "          1.0968e-01,  3.0356e-01, -5.0739e-03,  1.6671e-01,  4.9248e-01,\n",
       "          2.1848e-01, -3.5509e-01, -1.8390e-03, -2.2517e-01,  1.8351e-02,\n",
       "          2.4910e-01, -1.6452e-01,  1.9144e-01,  3.9998e-01,  3.0930e-01,\n",
       "          4.3670e-01,  1.7194e-02, -1.5155e-01,  1.0933e-01,  2.1600e-01,\n",
       "          5.0444e-02, -1.4622e-01, -1.6133e-01,  2.4824e-01,  2.3653e-02,\n",
       "         -1.3337e-01, -2.1521e-02, -1.0991e-01,  4.2818e-02, -1.2381e-01,\n",
       "         -3.7842e-01,  3.1447e-02,  2.0954e-01, -4.5253e-01,  9.8709e-02,\n",
       "         -2.8550e-01,  2.7720e-02, -2.2750e-01,  2.0080e-01, -2.2000e-01,\n",
       "         -9.7510e-02,  3.7623e-01, -1.0044e-01,  5.8680e-02, -2.0776e-01,\n",
       "         -1.4328e-01,  2.0717e-02,  1.0524e-03, -1.4229e-02, -2.7829e-02,\n",
       "          3.5721e-01, -1.3155e-01,  1.3367e-02,  2.6793e-02,  2.3148e-01,\n",
       "         -6.0900e-02,  2.1761e-01,  3.5376e-02, -1.4166e-01, -3.8733e-01,\n",
       "          1.4728e-01, -2.1783e-01, -4.2170e-01, -3.5043e-01,  3.5682e-01,\n",
       "         -1.2440e-01, -2.5066e-01, -1.9396e-01, -2.5677e-01,  8.8020e-02,\n",
       "          1.6871e-01,  4.6113e-01, -3.7811e-01, -7.6228e-02,  4.7493e-01,\n",
       "         -5.1578e-02, -1.7097e-01,  2.9060e-01,  2.2227e-01, -3.3062e-01,\n",
       "          3.1577e-01,  2.6889e-01, -3.6400e-02,  2.5280e-02,  5.0951e-01,\n",
       "          1.2886e-01,  1.8619e-01, -2.2398e-01,  4.3652e-01, -1.9887e-01,\n",
       "          3.1790e-01, -1.3419e-01, -2.2571e-01, -1.9172e-01,  3.1991e-03,\n",
       "          3.1575e-01,  1.7577e-01, -4.1601e-01, -9.6260e-02,  2.4071e-02,\n",
       "          3.4203e-01, -3.6020e-01, -1.2041e-01,  1.3803e-02, -3.4481e-01,\n",
       "          1.3344e-01,  9.6780e-02,  2.2914e-01, -3.7080e-01,  8.2148e-03,\n",
       "          3.8054e-01, -2.9618e-01,  1.1403e-01,  3.2114e-01,  7.4215e-02,\n",
       "          3.5050e-01, -2.6186e-02,  3.7600e-03,  5.7328e-02, -2.3499e-01,\n",
       "         -2.7944e-02,  1.3851e-01,  5.3372e-01,  1.3975e-01, -3.6947e-01,\n",
       "          7.1932e-02,  2.4091e-01, -1.3518e-01,  3.1758e-01, -7.8236e-02,\n",
       "         -7.3708e-02,  2.4197e-01, -6.4496e-02,  1.5763e-01, -8.1234e-02,\n",
       "         -2.4235e-01, -3.1835e-01,  3.5765e-01, -2.2095e-01, -1.1646e-01,\n",
       "         -1.6130e-01, -9.5469e-02, -1.2466e-01,  6.1398e-02, -3.8428e-01,\n",
       "          3.2753e-01,  1.3370e-01, -1.7987e-01, -1.0315e-01, -7.8941e-02,\n",
       "         -1.3857e-01, -2.2309e-01, -2.3365e-01,  4.1471e-01, -1.7748e-01,\n",
       "         -4.4335e-01,  2.4710e-01,  4.9943e-02,  3.3552e-01,  2.5792e-02,\n",
       "          1.0305e-01, -4.5467e-02,  1.3883e-01,  9.4557e-02, -9.8949e-02,\n",
       "          2.7854e-01,  4.4913e-02, -5.5480e-01, -1.4453e-01, -2.0310e-01,\n",
       "          7.2529e-02,  1.9152e-01, -3.1825e-01,  2.8978e-02,  2.1626e-02,\n",
       "          1.1347e-01,  2.6239e-02, -9.7462e-02, -6.9106e-02,  3.8603e-01,\n",
       "          2.2647e-01,  2.7475e-01,  9.2419e-02,  2.2696e-01, -2.3975e-03,\n",
       "         -2.9978e-01,  4.5256e-02,  6.1914e-02, -1.8714e-01,  4.2388e-01,\n",
       "         -8.8241e-02, -3.8105e-01, -9.1173e-02,  3.9800e-01,  1.0473e-01,\n",
       "         -4.5887e-02, -5.0480e-02,  1.9246e-01,  1.4535e-01, -1.1340e-01,\n",
       "          1.5864e-01, -3.4579e-02, -1.5416e-01, -1.1436e-01,  9.9272e-02,\n",
       "         -2.2930e-01,  7.1790e-02, -1.3824e-01, -1.8481e-02, -2.0938e-01,\n",
       "          7.1698e-03, -1.8171e-01,  2.4607e-01, -3.2995e-01,  1.1660e-01,\n",
       "          9.8278e-02,  2.9766e-01, -3.3704e-01, -1.6636e-01, -4.5362e-02,\n",
       "          1.4894e-01,  2.6992e-01,  3.4690e-01,  1.8040e-02,  3.7723e-02,\n",
       "         -1.6437e-01, -2.4633e-01,  7.9288e-02, -1.9933e-01,  1.4413e-01,\n",
       "          4.2749e-02,  2.2264e-01, -3.1898e-01, -1.8588e-01,  2.1133e-01,\n",
       "         -8.4487e-02, -1.5035e-01,  4.1283e-01,  2.4928e-01,  2.0458e-01,\n",
       "          1.8077e-02,  2.4015e-01,  4.3007e-02, -1.8276e-01, -1.1524e-01,\n",
       "         -2.6951e-01,  7.0473e-02, -7.5879e-02, -5.0248e-02, -5.8365e-02,\n",
       "         -1.1355e-01, -1.9441e-01, -1.6509e-01,  1.6267e-01,  1.2274e-01,\n",
       "          5.6327e-03, -4.2395e-02, -1.5045e-02, -2.5761e-01,  3.1590e-01,\n",
       "          2.0639e-02,  6.7995e-02, -5.1435e-02,  3.5629e-02, -1.4861e-01,\n",
       "          2.2416e-01,  2.2426e-01,  9.4583e-02, -1.7588e-01, -4.2027e-02,\n",
       "         -2.8940e-01, -3.5059e-01,  6.8860e-02,  1.3461e-01,  1.0860e-01,\n",
       "         -7.3878e-02, -2.7460e-01,  8.2261e-03, -1.2105e-01,  1.6309e-01,\n",
       "          1.7179e-02, -1.3059e-01, -7.8964e-02, -4.9359e-02, -4.8705e-02,\n",
       "          6.1416e-02, -1.8651e-01, -1.8565e-01, -8.8171e-02, -6.7246e-02,\n",
       "         -6.7304e-02,  3.5998e-01, -1.8936e-02,  2.7350e-01, -1.6029e-01,\n",
       "          1.3542e-02, -1.5854e-01,  1.1307e-01, -8.2161e-02,  6.5485e-02,\n",
       "          2.6472e-01, -4.3141e-01, -1.7239e-01, -1.2834e-05, -2.0636e-01,\n",
       "         -1.3022e-01, -6.5641e-02, -3.7094e-02,  2.2025e-01, -3.3416e-01,\n",
       "          2.3243e-01, -8.0841e-02,  1.8476e-01, -6.9578e-02, -2.2781e-01,\n",
       "         -1.4783e-01,  1.1704e-03,  2.4644e-01, -3.3969e-01, -2.5260e-01,\n",
       "         -2.7983e-01, -8.3401e-02, -5.9954e-02, -2.5297e-01,  4.1682e-01,\n",
       "         -1.1590e-01, -9.5772e-02,  3.7687e-02,  4.4791e-01,  1.9704e-01,\n",
       "          1.4975e-01,  1.9341e-01, -1.4506e-02,  9.9741e-03,  1.0676e-01,\n",
       "         -4.6339e-01,  2.0677e-01, -2.1781e-01, -1.3112e-01,  3.7147e-02,\n",
       "          6.8824e-02, -1.3885e-02,  2.1615e-03, -1.3199e-01, -8.7134e-02,\n",
       "          2.1300e-01, -3.6958e-01, -2.7301e-02,  2.6020e-01,  1.4791e-01,\n",
       "         -2.2676e-01,  3.0806e-02,  1.2233e-01,  3.7937e-01,  8.8595e-02,\n",
       "         -2.1603e-01,  1.1791e-01, -3.4336e-01, -1.0951e-02, -1.8886e-01,\n",
       "         -2.9609e-01,  1.5042e-01, -5.5640e-02,  9.5796e-02, -9.5458e-02,\n",
       "         -3.0377e-01,  2.1318e-01, -4.0907e-02, -8.7687e-02,  4.1051e-01,\n",
       "          7.3075e-03, -1.1577e-01,  1.4143e-01,  1.3236e-02,  2.8849e-02,\n",
       "         -9.7795e-02,  2.6043e-01,  1.9421e-01, -2.6599e-01,  1.5474e-01,\n",
       "         -1.1110e-01, -4.8496e-02, -9.1286e-02]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # inference on input\n",
    "# output = model(input_ids=ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "\n",
    "output = model(ids, mask)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "04d2a2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['last_hidden_state', 'pooler_output'])\n",
      "torch.Size([1, 350, 768])\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(output.keys())\n",
    "print(output[\"last_hidden_state\"].shape)\n",
    "print(output[\"pooler_output\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-transportation",
   "metadata": {
    "papermill": {
     "duration": 0.025529,
     "end_time": "2021-05-16T08:14:43.786750",
     "exception": false,
     "start_time": "2021-05-16T08:14:43.761221",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "2 outputs (last_hidden_state, pooler_output) from RoBERTa  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "significant-orbit",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-16T08:14:43.837616Z",
     "iopub.status.busy": "2021-05-16T08:14:43.836938Z",
     "iopub.status.idle": "2021-05-16T08:14:43.842607Z",
     "shell.execute_reply": "2021-05-16T08:14:43.842073Z"
    },
    "papermill": {
     "duration": 0.032211,
     "end_time": "2021-05-16T08:14:43.842766",
     "exception": false,
     "start_time": "2021-05-16T08:14:43.810555",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([1, 350, 768])\n",
      "shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# last_hidden_state & pooler output\n",
    "last_hidden_state = output[0] \n",
    "pooler_output     = output[1]\n",
    "print(\"shape:\", last_hidden_state.shape)\n",
    "print(\"shape:\", pooler_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-african",
   "metadata": {
    "papermill": {
     "duration": 0.024193,
     "end_time": "2021-05-16T08:14:43.891103",
     "exception": false,
     "start_time": "2021-05-16T08:14:43.866910",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "350 = num of tokens in a text, 768 = dimension of word embedding  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d39a74",
   "metadata": {
    "papermill": {
     "duration": 0.025272,
     "end_time": "2021-05-16T08:14:44.002185",
     "exception": false,
     "start_time": "2021-05-16T08:14:43.976913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "meaning of pooler output will be explained later.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-cause",
   "metadata": {
    "papermill": {
     "duration": 0.02598,
     "end_time": "2021-05-16T08:14:44.054123",
     "exception": false,
     "start_time": "2021-05-16T08:14:44.028143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Get CLS Token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-orlando",
   "metadata": {
    "papermill": {
     "duration": 0.025948,
     "end_time": "2021-05-16T08:14:44.106030",
     "exception": false,
     "start_time": "2021-05-16T08:14:44.080082",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "![get cls token](https://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "healthy-sacramento",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-05-16T08:14:44.167032Z",
     "iopub.status.busy": "2021-05-16T08:14:44.165983Z",
     "iopub.status.idle": "2021-05-16T08:14:44.174995Z",
     "shell.execute_reply": "2021-05-16T08:14:44.173517Z"
    },
    "papermill": {
     "duration": 0.044246,
     "end_time": "2021-05-16T08:14:44.175147",
     "exception": false,
     "start_time": "2021-05-16T08:14:44.130901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([1, 768])\n",
      "1 = num of texts, 768 = dimension of text embedding\n",
      "\n",
      "tensor([[-5.6703e-02,  6.6869e-02, -6.1948e-03, -1.0451e-01,  6.5941e-02,\n",
      "         -8.7096e-02, -6.3141e-02,  2.5446e-02,  6.8636e-02, -7.0920e-02,\n",
      "         -2.4731e-02,  5.4767e-02,  4.4447e-02, -1.2294e-02,  7.7633e-02,\n",
      "          2.3673e-02, -5.1255e-02,  1.7971e-02,  5.3081e-02, -4.0974e-02,\n",
      "         -1.0916e-01,  1.2808e-02,  2.7878e-02,  9.9498e-02,  1.4707e-02,\n",
      "          1.0225e-02,  1.0518e-01,  8.4490e-02, -5.7642e-02, -5.0169e-02,\n",
      "         -2.7050e-02, -2.5857e-02,  1.8748e-02, -4.6684e-02,  4.2878e-02,\n",
      "          7.8247e-02,  7.9697e-02,  1.0175e-02, -7.2391e-02, -1.2638e-02,\n",
      "         -3.3176e-02,  6.5563e-02,  1.3120e-02,  1.4457e-02,  5.8819e-02,\n",
      "          5.5081e-02,  2.7548e-02,  3.7400e-02, -3.1940e-02,  2.8428e-02,\n",
      "          4.2821e-02,  7.3411e-02, -5.3415e-02,  1.3923e-02, -8.1630e-02,\n",
      "          4.3756e-02, -2.8308e-03,  7.0701e-02,  1.5391e-02, -3.0722e-02,\n",
      "          6.7207e-02, -1.2232e-01, -9.2918e-02, -1.8306e-02,  1.3194e-02,\n",
      "         -2.7614e-02, -5.4690e-02, -2.7162e-02,  3.7386e-02,  1.0501e-02,\n",
      "          9.0274e-02, -7.1836e-02,  5.4011e-02, -3.7813e-02,  3.8490e-03,\n",
      "         -2.8183e-02,  2.8399e-02,  4.6086e-01, -2.8184e-02, -6.9914e-03,\n",
      "          7.7510e-02, -5.7535e-02,  3.2173e-01,  1.0369e-02,  2.7899e-02,\n",
      "          3.5680e-02,  7.0318e-02,  2.3173e-02,  2.0594e-02,  5.7544e-02,\n",
      "          1.6017e-02,  8.7675e-02, -7.3520e-02,  5.7131e-03, -1.4788e-02,\n",
      "          6.1403e-02,  3.5443e-03, -5.7710e-02, -2.9487e-02, -4.6080e-02,\n",
      "         -1.1624e-02, -6.7018e-02,  1.0307e-01,  3.5309e-02,  1.1483e-02,\n",
      "         -2.1405e-02,  4.7751e-02, -2.4853e-02,  3.5766e-02, -6.9396e-02,\n",
      "          9.9671e-03,  6.1038e-02,  1.4359e-03,  8.7279e-02, -3.3974e-02,\n",
      "         -4.3325e-02,  1.0206e-02, -1.0623e-03, -2.5272e-02,  4.9466e-02,\n",
      "          2.8172e-02,  5.8568e-02,  1.0542e-01, -2.7114e-02, -2.7577e-02,\n",
      "          5.8653e-03,  8.2543e-03, -1.3236e-02, -5.7633e-02,  8.3806e-02,\n",
      "         -9.8550e-03, -1.8708e-02, -5.2719e-03,  8.1796e-02,  6.7613e-02,\n",
      "          5.7566e-02,  5.0079e-02,  3.6934e-03,  4.7156e-02,  5.5305e-03,\n",
      "         -1.4000e-02,  7.6552e-02,  8.9096e-02,  4.9037e-02,  5.7585e-02,\n",
      "         -8.6816e-03,  6.9022e-02, -4.6858e-02,  1.9967e-02, -2.6716e-02,\n",
      "          6.4981e-02, -6.1103e-02, -3.8217e-02,  5.0337e-02,  1.0286e-02,\n",
      "          3.6083e-01,  5.3056e-02,  3.9346e-02, -3.4418e-02, -1.9350e-03,\n",
      "          1.4802e-01, -1.1503e-02,  3.9178e-03,  2.2427e-02, -2.4205e-02,\n",
      "          1.5354e-02,  3.6329e-02,  2.4876e-02,  7.0831e-02, -1.9473e-02,\n",
      "          8.8882e-02,  2.7500e-02,  6.2889e-02, -2.0710e-02, -1.5240e-02,\n",
      "         -6.5315e-02, -5.0906e-02, -4.1502e-02, -9.0786e-02, -2.6040e-02,\n",
      "          7.6591e-02,  6.3936e-02, -7.7138e-02,  1.0828e-03, -3.3113e-02,\n",
      "          3.6666e-02, -1.1787e-02,  3.5266e-02, -7.4842e-03,  1.0183e-02,\n",
      "          5.2111e-02, -6.2060e-02, -3.2508e-02, -9.7531e-03, -4.3033e-03,\n",
      "          9.4525e-02, -2.8592e-02, -4.5592e-02,  1.6745e-02, -2.5331e-02,\n",
      "          4.3659e-02, -1.0296e-01,  5.0351e-02, -7.0393e-02,  5.0011e-02,\n",
      "         -1.5393e-02,  9.2536e-03,  4.4844e-02,  4.1662e-02, -5.8417e-02,\n",
      "         -7.0148e-02,  6.0245e-02,  2.4910e-02,  5.0201e-02,  1.4352e-02,\n",
      "          5.4751e-03,  5.9690e-02,  1.1798e-01,  1.0200e-03, -2.1686e-02,\n",
      "          1.5633e-02,  6.1002e-02, -9.8768e-03,  1.1278e-01, -8.7282e-04,\n",
      "         -1.2749e-02, -2.6134e-03,  2.6530e-02,  1.4110e-02, -1.6216e-02,\n",
      "         -2.3382e-02,  1.0625e-02,  5.8663e-02, -3.4593e-02,  7.8071e-02,\n",
      "         -1.4881e-01,  7.0234e-03, -7.1794e-03, -1.2146e-02,  4.9612e-02,\n",
      "         -1.3417e-01,  8.4249e-03,  3.4957e-02,  2.7312e-02, -5.8635e-03,\n",
      "          4.1574e-02,  2.7679e-02,  5.1578e-02, -2.3717e-02,  6.6516e-02,\n",
      "         -4.3951e-02,  2.0716e-02, -1.9507e-02, -1.2317e-02,  2.2644e-02,\n",
      "         -2.5798e-02, -1.0983e-01, -5.9349e-03,  5.7348e-02,  2.4300e-02,\n",
      "         -8.0060e-03, -3.5833e-02, -1.4510e-02, -2.2327e-02, -1.2335e-01,\n",
      "         -7.1707e-02, -6.7132e-02,  1.1409e-02, -3.2056e-02, -4.4836e-02,\n",
      "         -9.5107e-03, -4.5132e-02, -4.2983e-02,  7.3366e-03,  3.1999e-02,\n",
      "          5.2781e-02, -2.5616e-02, -3.1355e-02, -9.7520e-02, -1.5698e-02,\n",
      "          1.6785e-02,  7.0195e-03, -6.8384e-02,  1.8657e-02,  1.8710e-02,\n",
      "         -1.6757e-02,  1.6008e-02, -3.4225e-02,  5.5745e-02,  1.5358e-02,\n",
      "          4.4975e-02,  7.7368e-03, -2.0404e-02,  2.3904e-02, -5.8139e-02,\n",
      "          4.4604e-03,  7.3939e-02, -4.2433e-03,  4.0405e-02, -2.1303e-02,\n",
      "         -7.3034e-03, -8.8540e-02, -8.9183e-02, -1.8086e-02,  6.0729e-02,\n",
      "         -3.9907e-02, -1.4284e-02,  3.6751e-03,  6.8544e-02,  1.9550e-02,\n",
      "         -5.8484e-02, -2.4026e-02, -1.5257e-01,  1.4048e-01, -7.6227e-03,\n",
      "         -4.5602e-02,  1.7141e-02, -1.8388e-03,  2.6991e-02,  7.9299e-02,\n",
      "          1.0013e-03,  1.0414e-02,  6.4515e-02,  1.0329e-02, -3.6566e-02,\n",
      "         -1.1654e-03,  5.2652e-02, -3.9406e-02,  4.8000e-02,  4.0746e-01,\n",
      "         -2.7666e-01,  2.8238e-02,  5.2097e-02,  1.1711e-02,  8.1702e-02,\n",
      "          1.4029e-02,  2.6888e-02,  4.5495e-02,  5.8177e-02,  6.0662e-02,\n",
      "         -1.9701e-02,  3.2183e-02,  1.4136e-02, -1.6260e-02,  2.3426e-02,\n",
      "          5.0466e-04, -1.5838e-04, -1.1980e-02,  1.4639e-02,  2.7748e-02,\n",
      "         -2.3890e-03, -8.1070e-03,  3.3380e-02, -4.6672e-02, -4.5094e-03,\n",
      "          3.9876e-02,  5.4077e-02, -1.2682e-02, -1.2765e-02, -3.1878e-02,\n",
      "          7.5869e-02,  3.3115e-02,  5.6439e-02, -4.0491e-02,  1.0102e-01,\n",
      "         -7.3100e-02, -4.7613e-02,  8.3622e-02,  1.6940e-03,  4.8001e-03,\n",
      "          5.7303e-02,  5.0118e-03,  1.9984e-02, -9.7266e-03,  1.0584e-02,\n",
      "         -1.2968e-02,  3.4016e-02,  1.5474e-02,  3.1509e-02,  9.2730e-02,\n",
      "          5.2164e-02,  3.0643e-02, -4.8557e-02,  4.7833e-02,  7.8879e-02,\n",
      "         -2.6255e-02,  2.6396e-02,  3.7597e-02,  9.5518e-02,  2.7958e-02,\n",
      "         -4.8626e-02, -1.0631e-01,  1.9560e-02, -4.0151e-02,  1.0852e-01,\n",
      "          2.0066e-02, -2.2430e-02, -1.9777e-01, -5.8598e-02, -4.6595e-02,\n",
      "          4.0605e-02,  2.3569e-02, -1.8115e-02, -6.1764e-03,  4.7672e-02,\n",
      "         -2.2813e-02, -8.3385e-03, -6.6613e-02, -3.4838e-02,  2.2507e-02,\n",
      "         -9.2725e-03,  3.8746e-02,  3.2769e-02, -3.0380e-02,  1.4894e-02,\n",
      "         -1.5000e-02,  4.3922e-02,  1.4698e-02, -9.0872e-04,  1.7213e-03,\n",
      "         -4.3563e-02,  4.6666e-02,  9.6659e-03,  9.6980e-03, -7.0305e-02,\n",
      "          9.6303e-04,  2.1986e-02, -9.3419e-02, -1.7492e-02, -7.1310e-03,\n",
      "          9.2381e-03,  4.8944e-02, -4.6758e-02,  2.7181e-03, -2.3700e-02,\n",
      "         -1.8127e-02,  2.7973e-02, -2.7345e-02, -2.5783e-02, -3.5195e-02,\n",
      "         -4.8775e-02, -3.8259e-03,  8.1284e-02, -1.9410e-02, -1.0571e-01,\n",
      "          5.8085e-03,  4.7939e-02,  3.9017e-02, -5.5830e-02, -5.4654e-03,\n",
      "          1.4995e-02,  6.9279e-02, -3.4309e-02, -7.3570e-01,  6.5216e-02,\n",
      "          6.1738e-02,  2.4686e-02,  9.6216e-03, -6.5523e-02, -2.8916e-02,\n",
      "          2.1062e-02,  6.1131e-02,  3.3884e-02, -2.1191e-02, -2.5611e-02,\n",
      "          2.6237e-03, -5.1916e-02,  7.5761e-02, -3.6293e-02, -4.5435e-02,\n",
      "          6.9265e-02,  8.5220e-03, -2.9524e-02, -3.7002e-02,  9.1800e-04,\n",
      "         -2.6366e-02, -5.5388e-02,  5.9828e-02, -5.5776e-02, -5.3906e-02,\n",
      "         -4.6212e-03,  6.6033e-02, -2.3880e-03, -2.0319e-02, -9.2402e-02,\n",
      "          1.2320e-02, -2.2411e-02,  5.7431e-02,  4.5754e-02,  7.6005e-02,\n",
      "         -1.8292e-02,  1.2242e-02, -3.1560e-03,  6.1074e-02,  2.2563e-01,\n",
      "          4.2533e-03,  1.7998e-01, -1.9528e-02, -5.0441e-02, -1.1941e-02,\n",
      "          2.7987e-02, -6.9996e-02, -2.6300e-02, -1.1034e-03,  7.1043e-03,\n",
      "         -1.3734e-02, -5.8870e-02, -4.4827e-02,  1.1848e-02,  5.2682e-02,\n",
      "         -4.3516e-02,  9.3293e-02,  1.1013e-01, -1.5613e-02,  9.7174e-03,\n",
      "          3.8202e-02,  3.9934e-02, -2.0898e-02, -3.7791e-02,  2.5700e-02,\n",
      "         -3.9774e-02,  1.3834e-02, -2.9875e-02, -5.4841e-02, -6.2999e-02,\n",
      "         -4.2953e-02,  1.4025e-02,  1.5521e-02,  1.6802e-02,  3.8096e-02,\n",
      "          1.7270e-03, -7.4924e-02,  6.6542e-02, -6.7991e-03,  5.1295e-02,\n",
      "          1.4217e-02,  7.8667e-02,  3.7553e-02, -2.9717e-02, -6.8280e-02,\n",
      "         -3.5454e-02,  1.6460e-02,  9.6473e-02, -6.0813e-04,  6.4600e-02,\n",
      "          2.0505e-02,  3.0388e-03, -4.9237e-03,  3.5965e-02,  5.9028e-02,\n",
      "         -1.6063e-03, -6.1421e-01, -5.6206e-02,  6.7012e-02,  3.6245e-02,\n",
      "          4.1114e-02,  3.0015e-02, -4.1690e-03, -4.1692e-02,  9.1067e-02,\n",
      "         -4.0473e-02,  4.5099e-02,  3.5878e-02,  5.4661e-02, -4.6211e-02,\n",
      "          5.1285e-02,  4.8053e-02, -5.1985e-02, -7.9963e-02,  3.8122e-02,\n",
      "         -3.2212e-01, -3.0769e-02, -6.6290e-02,  8.6604e-02, -2.4904e-02,\n",
      "          3.3506e-02,  3.1279e-02, -8.4687e-03,  2.1195e-02,  9.2624e-02,\n",
      "          2.8917e-02,  5.8687e-02,  5.4912e-02, -2.5554e-02,  1.1343e-02,\n",
      "         -1.5100e-02,  8.0719e-02,  2.5494e-02,  1.0592e+01, -2.4014e-02,\n",
      "          3.1995e-02,  2.1700e-02, -3.1254e-02, -1.0390e-01,  4.2203e-02,\n",
      "         -3.4941e-02,  2.2702e-02,  6.5998e-02,  6.3620e-03, -9.4185e-03,\n",
      "         -5.7244e-02, -1.5553e-02, -7.3133e-03,  1.6612e-02, -9.8448e-02,\n",
      "         -2.9125e-02,  6.5343e-02, -9.8565e-03,  4.0793e-02,  1.2445e-02,\n",
      "         -1.5061e-03,  2.2540e-02, -7.6621e-02,  3.4129e-04, -4.6015e-03,\n",
      "         -2.1801e-02, -2.6575e-02,  1.4414e-02,  2.4152e-02,  7.8327e-03,\n",
      "          3.3625e-02,  1.0959e-02,  3.8859e-02, -4.9374e-03, -5.0758e-02,\n",
      "          5.8135e-02,  3.1043e-02,  9.7229e-02,  9.3471e-02,  2.7602e-03,\n",
      "         -2.1938e-02, -2.1036e-02,  4.4204e-02,  4.7900e-02, -6.4483e-02,\n",
      "          6.9241e-02, -3.0673e-03,  3.5855e-02,  3.8534e-02, -9.3778e-02,\n",
      "          4.4469e-02,  2.8939e-02, -3.9084e-02, -8.8856e-03,  3.6320e-02,\n",
      "         -6.2103e-02,  5.3165e-02,  5.8428e-02, -9.8873e-03,  1.0756e-01,\n",
      "          7.5842e-03,  7.3007e-03, -1.1359e-01,  5.4707e-02,  5.0121e-02,\n",
      "          8.4946e-02, -2.6988e-02,  4.3262e-02,  6.3269e-02, -5.8627e-02,\n",
      "         -4.0436e-02, -2.9976e-02,  1.9812e-02, -3.9000e-02,  3.8965e-02,\n",
      "         -5.7527e-02,  4.8677e-02,  1.6553e-02,  6.2798e-02,  5.0676e-02,\n",
      "         -8.7135e-03, -1.9645e-02,  3.1402e-02,  2.0603e-02,  7.8640e-02,\n",
      "          2.2455e-02,  3.5549e-02, -5.0681e-03, -6.0120e-02,  2.5101e-02,\n",
      "          2.2578e-02, -5.5439e-02,  3.4685e-02,  3.2828e-02,  2.4622e-02,\n",
      "         -1.1004e-01,  1.0174e-01,  4.0280e-02, -4.9709e-02,  2.2341e-02,\n",
      "         -1.0540e-02,  2.1700e-02,  2.8294e-02,  3.4277e-02,  9.6568e-03,\n",
      "         -1.0514e-02,  3.6488e-02,  2.2924e-02, -1.5338e-02,  1.0927e-02,\n",
      "          4.3155e-03,  1.2687e-02, -1.3716e-02,  5.3010e-02,  2.4393e-03,\n",
      "          2.9295e-02,  1.6047e-02,  3.6133e-02, -9.3520e-02,  3.3992e-02,\n",
      "         -1.2126e-02,  2.4307e-04, -4.0297e-02, -5.3119e-02,  7.5222e-04,\n",
      "         -4.2307e-02, -2.3711e-02, -1.9830e-02, -6.3731e-02,  4.6224e-02,\n",
      "          4.8740e-02,  8.2787e-03, -1.1887e-02,  2.2407e-02,  1.7638e-02,\n",
      "          6.9929e-02,  7.6752e-02,  8.4460e-02, -5.4735e-02,  9.6020e-03,\n",
      "         -4.4301e-03, -7.9456e-02,  5.1501e-02, -6.9814e-04,  9.0922e-02,\n",
      "          3.5178e-02,  4.6459e-02,  1.6967e-02,  3.7603e-02,  3.3602e-02,\n",
      "          4.1574e-02, -3.4903e-02,  6.2007e-02,  1.9975e-03, -4.5051e-02,\n",
      "          4.1420e-02,  1.8023e-02, -3.6831e-02,  5.3829e-02, -3.5060e-02,\n",
      "         -2.0147e-02,  3.0764e-02, -1.6366e-02, -7.1456e-03, -6.7479e-02,\n",
      "          6.2596e-02,  2.1795e-02, -2.5158e-03,  2.0911e-02, -2.9300e-02,\n",
      "         -9.8154e-02, -7.7925e-02,  1.5317e-02,  1.2096e-01,  8.5528e-02,\n",
      "         -1.2196e-01, -2.5583e-02, -1.3182e-02]])\n"
     ]
    }
   ],
   "source": [
    "# .detach() = make copies and remove gradient information  \n",
    "cls_embeddings = last_hidden_state[:, 0, :].detach()\n",
    "\n",
    "print(\"shape:\", cls_embeddings.shape)\n",
    "print(\"{} = num of texts, {} = dimension of text embedding\".format(cls_embeddings.shape[0],cls_embeddings.shape[1]))\n",
    "print(\"\")\n",
    "print(cls_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-spoke",
   "metadata": {
    "papermill": {
     "duration": 0.024556,
     "end_time": "2021-05-16T08:14:44.224871",
     "exception": false,
     "start_time": "2021-05-16T08:14:44.200315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "768 = dimension of text embedding  per text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entitled-forth",
   "metadata": {
    "papermill": {
     "duration": 0.025645,
     "end_time": "2021-05-16T08:14:44.370035",
     "exception": false,
     "start_time": "2021-05-16T08:14:44.344390",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Pool RoBERTa Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "atlantic-serve",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-16T08:14:44.480127Z",
     "iopub.status.busy": "2021-05-16T08:14:44.478763Z",
     "iopub.status.idle": "2021-05-16T08:14:44.484612Z",
     "shell.execute_reply": "2021-05-16T08:14:44.483944Z"
    },
    "papermill": {
     "duration": 0.037745,
     "end_time": "2021-05-16T08:14:44.484794",
     "exception": false,
     "start_time": "2021-05-16T08:14:44.447049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 350, 768])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dried-rebecca",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-05-16T08:14:44.544288Z",
     "iopub.status.busy": "2021-05-16T08:14:44.543550Z",
     "iopub.status.idle": "2021-05-16T08:14:44.555902Z",
     "shell.execute_reply": "2021-05-16T08:14:44.556375Z"
    },
    "papermill": {
     "duration": 0.044733,
     "end_time": "2021-05-16T08:14:44.556553",
     "exception": false,
     "start_time": "2021-05-16T08:14:44.511820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([1, 768])\n",
      "\n",
      "tensor([[ 2.0323e-02,  7.9437e-02, -1.1032e-01, -3.5171e-02,  2.2646e-01,\n",
      "         -2.7637e-02, -7.7746e-02, -1.4751e-05,  9.8366e-02, -4.2963e-02,\n",
      "         -5.9774e-02, -8.9828e-03, -1.1668e-01, -1.1539e-03, -5.1606e-02,\n",
      "          4.5606e-01,  3.0563e-01,  1.5051e-01,  3.0782e-02,  2.1871e-01,\n",
      "          5.1562e-02,  2.3083e-02,  4.0371e-01, -1.3139e-01,  1.3204e-01,\n",
      "         -8.1892e-02,  2.3483e-01,  6.0469e-02,  1.4729e-02, -1.1617e-01,\n",
      "          8.0583e-02,  6.4143e-02, -2.5171e-01, -2.0018e-01,  5.6423e-02,\n",
      "          3.3663e-02, -1.1430e-02,  1.8541e-02,  3.3345e-01, -9.3171e-02,\n",
      "         -1.1654e-01,  2.2614e-01,  1.1700e-01, -4.0023e-02,  1.1706e-01,\n",
      "         -6.9963e-02,  3.8883e-02,  4.5444e-01, -8.1482e-04,  1.7505e-01,\n",
      "          1.0154e-01, -1.3573e-01, -1.6017e-02,  3.5862e-03,  6.8536e-02,\n",
      "          1.3999e-01, -5.0224e-02, -1.3244e-01, -1.3408e-01,  5.0157e-03,\n",
      "          1.9436e-01, -1.8392e-01, -8.8699e-02, -2.2767e-02,  1.0509e-02,\n",
      "         -5.8541e-02,  3.8943e-02,  7.4841e-03, -1.8606e-01, -3.4022e-03,\n",
      "          1.2389e-01, -2.8656e-01,  3.2529e-02,  2.6665e-02,  9.9526e-02,\n",
      "         -2.3970e-01,  1.9252e-01, -4.7866e+00, -1.3475e-01,  2.4634e-02,\n",
      "         -5.2514e-05, -2.0905e-01, -1.8300e+00, -4.5689e-02, -3.0736e-02,\n",
      "          2.6734e-01,  5.3346e-02,  5.0606e-02,  3.5966e-02,  6.3563e-02,\n",
      "          4.3756e-02,  5.9892e-02,  6.0798e-02,  2.0692e-01, -1.5670e-01,\n",
      "          1.1381e-01,  8.2930e-02,  6.5995e-02, -1.2964e-01,  1.3056e-01,\n",
      "          1.0033e-02, -8.6466e-02,  1.8829e-01, -1.1813e-01, -3.1809e-02,\n",
      "         -1.5359e-01,  3.6622e-02, -1.4957e-02,  1.1845e-01, -3.6851e-02,\n",
      "         -5.9052e-02,  8.8161e-03, -1.2594e-01,  7.0197e-02, -1.4772e-01,\n",
      "          1.1466e-01,  8.2927e-03,  1.3873e-02, -1.4990e-01,  2.0128e-01,\n",
      "         -3.6731e-02, -4.9162e-02, -2.3457e-02, -3.0277e-02, -4.4135e-02,\n",
      "          1.4090e-01,  6.8588e-02, -1.5621e-01, -6.8993e-02,  1.3737e-01,\n",
      "         -1.4486e-03,  3.8049e-01,  4.9714e-02, -1.7769e-02,  6.4223e-02,\n",
      "          1.2659e-01,  1.3129e-01,  2.3188e-01, -3.8953e-02,  2.3731e-04,\n",
      "          1.1995e-02,  1.4815e-01,  1.2170e-01, -5.1344e-02, -4.6211e-02,\n",
      "         -3.3489e-02,  2.6715e-01,  1.0738e-02,  2.8133e-03, -1.2782e-01,\n",
      "         -5.7255e-02,  1.0956e-01, -5.6527e-03,  1.6130e-01, -1.4723e-01,\n",
      "         -5.6082e-01,  8.1826e-02,  2.5779e-01, -3.4652e-02, -8.9882e-02,\n",
      "          4.6978e-02, -1.0108e-01, -1.5031e-02,  1.3712e-01, -9.0742e-04,\n",
      "          6.9476e-02,  1.5742e-01,  5.4789e-02,  1.1121e-01, -2.1576e-01,\n",
      "          2.7354e-02, -6.2911e-02,  5.9046e-02,  8.2967e-02,  1.2202e-01,\n",
      "         -1.3924e-01, -1.0305e-01, -1.1636e-01, -1.2637e-02,  7.8453e-02,\n",
      "         -1.2819e-01,  1.9243e-01, -1.2168e-01, -5.7990e-02,  1.3665e-01,\n",
      "         -3.4386e-02, -2.0958e-01, -1.4565e-01,  7.5746e-02, -9.5652e-02,\n",
      "          1.0852e-01, -1.2087e-01, -4.6708e-02,  5.2989e-03, -1.4639e-02,\n",
      "          5.0346e-02,  4.8519e-02, -2.6666e-04,  2.4958e-02, -8.6895e-02,\n",
      "          4.7943e-03, -2.8139e-01, -1.6477e-01,  1.8729e-02,  2.8139e-01,\n",
      "         -6.2619e-02, -1.0307e-01,  3.1894e-02,  6.0858e-02, -1.8674e-01,\n",
      "          2.1108e-02,  2.2986e-01,  1.6393e-01, -1.7803e-01,  8.6865e-02,\n",
      "         -3.3508e-02,  1.2015e-01, -4.6650e-01,  6.0153e-02, -8.9162e-03,\n",
      "         -7.2374e-02,  1.3946e-01,  2.3412e-02,  2.2618e-01, -2.0241e-02,\n",
      "          8.5242e-02,  1.3986e-02,  2.1169e-01, -4.5457e-02, -2.3128e-02,\n",
      "         -2.5407e-02, -6.1046e-02,  5.1430e-02, -2.7377e-01,  2.1101e-01,\n",
      "         -1.4343e-01,  1.8793e-02,  3.3846e-02, -6.4712e-02,  8.5443e-02,\n",
      "          4.3706e-01, -5.0801e-04,  1.9264e-01,  7.7603e-02, -5.9127e-02,\n",
      "          1.3556e-01,  7.0578e-02,  2.9452e-01,  1.1862e-01,  1.7505e-01,\n",
      "         -1.0733e-01,  3.9632e-02,  1.0152e-01,  5.9892e-02, -9.2302e-03,\n",
      "          1.5065e-01, -1.5630e-02,  3.9491e-03,  1.5405e-01, -2.8058e-01,\n",
      "         -6.4269e-03,  6.8317e-02, -1.3800e-01, -1.2566e-01, -1.0110e-01,\n",
      "          7.3270e-03,  1.4929e-03,  3.3083e-02, -1.8013e-01,  6.7710e-02,\n",
      "         -3.6274e-02,  7.0049e-03, -5.9014e-02, -1.0125e-04,  3.4993e-02,\n",
      "          1.4482e-01,  7.2717e-02,  3.3734e-02, -3.5814e-01, -2.5542e-02,\n",
      "          5.9989e-02,  1.0053e-01, -1.6182e-01,  8.5669e-02,  7.8868e-02,\n",
      "         -2.9451e-01,  3.0925e-01, -7.6854e-02,  4.0078e-02, -1.1631e-01,\n",
      "          1.6863e-01, -3.1351e-02, -7.7942e-02,  1.6719e-01,  3.7881e-02,\n",
      "         -5.6542e-02, -6.1730e-02, -1.9533e-01,  9.0050e-02, -2.5571e-01,\n",
      "          2.2916e-01, -1.5668e-02, -9.6241e-02,  4.1238e-02,  2.1876e-01,\n",
      "          5.1811e-02,  3.1207e-02,  8.9781e-02,  1.3179e-01,  3.1986e-01,\n",
      "          3.8120e-02,  1.2678e-02, -8.3316e-02,  2.1065e-01, -1.7917e-01,\n",
      "         -8.3417e-02, -1.9308e-01,  2.8886e-02, -6.4374e-02,  2.4308e-01,\n",
      "         -4.2323e-02, -7.0847e-02, -1.2642e-02,  8.6043e-02, -2.4700e-01,\n",
      "         -6.7006e-03, -4.6293e-02,  5.7117e-02, -2.8503e-02,  1.6667e-01,\n",
      "         -2.7738e-01,  4.6867e-01, -1.8206e-02, -1.6270e-01,  1.6048e-02,\n",
      "          4.7872e-02, -1.0982e-02,  1.3070e-01, -3.0093e-02,  5.6475e-02,\n",
      "          1.2753e-01, -5.8623e-03,  3.3999e-02, -8.4873e-02,  1.0154e-01,\n",
      "          1.7953e-01,  3.0866e-01, -1.3697e-01,  1.7122e-01,  1.1414e-02,\n",
      "          9.9464e-02, -2.4940e-02,  9.2629e-02,  6.8630e-03, -9.0920e-03,\n",
      "          4.9076e-02,  1.4417e-01,  1.2753e-01, -1.6706e-02, -3.3098e-01,\n",
      "          2.2457e-01,  4.9540e-01,  1.0501e-01, -1.4938e-01,  2.1973e-01,\n",
      "         -2.2023e-01, -6.9804e-02,  1.9819e-01, -5.2257e-02,  7.2740e-02,\n",
      "          1.6290e-01,  3.6443e-02,  7.4377e-02, -2.8846e-01, -6.1780e-03,\n",
      "         -1.5330e-01, -9.1144e-02,  1.8058e-01,  1.0233e-01, -2.2608e-02,\n",
      "          1.1431e-01,  1.5401e-01, -1.2916e-01,  6.3905e-02, -1.4603e-01,\n",
      "         -9.9150e-02, -1.7629e-02, -1.1314e-02,  4.5495e-01,  3.4147e-02,\n",
      "         -8.5458e-02, -1.3733e-01,  4.5159e-02, -3.7373e-03,  8.2743e-02,\n",
      "          9.2124e-02,  5.1363e-02, -4.0661e-01, -1.6999e-01,  1.2903e-01,\n",
      "         -3.4384e-02,  2.7792e-02, -7.6106e-02, -9.1752e-02,  2.5522e-01,\n",
      "         -2.7681e-03, -1.5811e-01, -1.2817e-01, -8.7532e-02,  1.1320e-01,\n",
      "          1.5133e-01,  1.7614e-01,  1.2924e-02,  7.8648e-02,  2.8990e-03,\n",
      "          1.2495e-02, -1.2031e-02,  9.5160e-02,  9.1543e-02, -1.9894e-02,\n",
      "          1.1065e-02, -5.4143e-03, -5.7743e-02, -8.5999e-02, -2.7809e-01,\n",
      "          1.0077e-01, -3.7271e-03, -1.7822e-02, -3.2708e-02,  6.6092e-02,\n",
      "          1.1353e-02, -7.4027e-02,  1.1398e-01,  1.6263e-01, -1.2523e-01,\n",
      "         -1.7314e-01,  2.3436e-02, -1.8804e-01,  2.0134e-02,  1.0270e-01,\n",
      "          4.7693e-02,  1.6521e-01,  2.3024e-01, -1.0775e-03, -1.9410e-01,\n",
      "         -1.9047e-02, -1.4163e-02,  2.6257e-02,  6.6088e-02,  9.1651e-02,\n",
      "         -1.2099e-01,  1.6651e-01,  8.5531e-02, -2.8424e+00,  1.7480e-01,\n",
      "          9.3246e-02,  1.1525e-01,  8.1036e-02, -1.0714e-02, -7.5824e-02,\n",
      "          1.4083e-02,  2.2293e-01, -3.2821e-02,  9.9155e-02, -1.3665e-01,\n",
      "          2.1442e-03,  6.4307e-02,  2.3697e-01, -1.6986e-01,  1.0957e-01,\n",
      "          9.1785e-02, -3.6617e-02, -6.6747e-02, -1.8526e-02,  6.7679e-02,\n",
      "          2.1873e-02, -1.0448e-01,  1.1409e-01,  1.3055e-01, -1.1478e-01,\n",
      "          3.5816e-04,  6.3210e-02, -4.9694e-02,  2.1566e-02,  2.1471e-03,\n",
      "          1.2549e-01, -2.4227e-01,  1.1116e-01, -5.7732e-02,  2.1919e-01,\n",
      "         -1.3835e-01,  9.1048e-02, -4.4900e-02,  3.2044e-01,  2.1534e-01,\n",
      "          7.4880e-02, -1.2466e-01,  3.2365e-02, -2.5585e-01, -1.1541e-01,\n",
      "          2.3298e-01, -9.8598e-03, -1.8411e-01, -7.0962e-02,  9.6812e-02,\n",
      "          3.2171e-02,  1.6106e-01, -1.1148e-01,  2.4477e-01, -3.9449e-02,\n",
      "         -1.7794e-02,  5.3986e-01, -1.9859e-02,  3.3132e-02,  4.8063e-02,\n",
      "          4.4450e-03,  2.0593e-01,  3.1955e-02, -7.9910e-02, -9.4655e-02,\n",
      "         -3.2691e-02, -1.2679e-01, -1.0872e-01, -8.6466e-02,  1.1686e-01,\n",
      "         -1.8607e-01,  3.3247e-02,  2.2108e-01, -1.2121e-01, -7.0144e-02,\n",
      "         -2.1277e-02, -3.1511e-01, -3.9070e-02,  1.1912e-01,  1.3822e-01,\n",
      "         -2.2889e-01, -3.3229e-02,  8.1434e-02,  4.7820e-02, -2.6472e-01,\n",
      "         -5.7184e-02,  8.4748e-02, -2.1887e-02,  1.3561e-01,  6.2569e-02,\n",
      "         -9.6890e-02,  8.6944e-03, -4.9850e-03,  9.7571e-02,  3.7758e-02,\n",
      "          1.8031e-01, -3.3199e-02,  1.0422e-01,  4.8772e-02,  1.8295e-01,\n",
      "          1.4446e-01,  2.8198e-02,  1.2972e-01, -3.2357e-02,  1.9741e-01,\n",
      "          1.0368e-01, -2.0056e-01,  1.0220e-01,  3.4289e-02,  2.8554e-02,\n",
      "          6.7393e-02,  8.3224e-02, -2.6406e-01, -8.5564e-02,  1.5128e-01,\n",
      "          2.0610e-01, -1.2104e-01, -1.5316e-01,  2.1405e-01, -6.7119e-02,\n",
      "          2.7891e-02,  3.4936e-02,  6.8900e-02, -3.2132e-02,  2.1555e-01,\n",
      "         -5.0864e-02,  1.3855e-01, -4.6576e-02,  7.3315e-02,  3.6063e-02,\n",
      "         -1.1202e-01, -2.2411e-03,  1.5975e-01,  8.7913e+00,  2.7346e-02,\n",
      "          9.3223e-02,  1.6581e-01, -1.7718e-01, -3.0334e-02,  9.3729e-03,\n",
      "         -2.6308e-01,  1.0752e-01,  1.0599e-01, -1.0518e-01, -2.1183e-01,\n",
      "          5.6840e-04, -3.7821e-02, -4.2327e-03,  1.1295e-02, -2.6275e-01,\n",
      "         -4.9590e-02,  2.0656e-02,  1.7984e-02,  8.1446e-02,  1.3913e-02,\n",
      "          3.3798e-03, -2.6950e-01, -4.8762e-02, -1.0746e-01, -5.1818e-02,\n",
      "         -2.4345e-02, -9.5981e-02,  1.9496e-01,  5.2274e-02,  2.7274e-01,\n",
      "          5.1142e-02, -3.9024e-02,  2.3710e-02,  6.7889e-03, -6.2860e-02,\n",
      "         -4.1365e-02,  1.6583e-01,  1.4163e-01,  6.9964e-02,  8.6375e-02,\n",
      "          1.0757e-01,  1.3773e-01,  2.3397e-02,  3.1523e-02, -2.1126e-02,\n",
      "          5.9000e-02, -3.1480e-02, -1.9888e-01,  5.0350e-02,  3.0114e-02,\n",
      "          5.7217e-02, -7.3787e-02, -1.1659e-01,  9.2130e-02, -1.9929e-01,\n",
      "         -1.5437e-01, -2.2178e-02,  3.0989e-01,  1.5806e-01,  1.0021e-01,\n",
      "          5.6865e-02, -2.4838e-02, -3.9811e-01, -6.6272e-02, -7.6579e-02,\n",
      "         -7.1555e-02, -1.3273e-02,  6.8539e-02,  2.1343e-01, -1.2178e-01,\n",
      "         -1.8420e-01,  6.0164e-02, -1.2035e-01,  8.7767e-02,  7.0969e-01,\n",
      "         -1.3406e-01,  1.0615e-01,  1.4873e-01,  1.2052e-02,  2.0712e-01,\n",
      "         -3.2839e-02, -2.7878e-02, -9.5625e-02,  1.3958e-02,  1.3001e-01,\n",
      "         -6.8338e-02,  2.2321e-01, -1.3127e-01, -1.1877e-01, -9.2359e-02,\n",
      "          8.4717e-02,  2.3631e-02, -8.0206e-02, -2.1308e-02,  1.9737e-01,\n",
      "         -3.8582e-02,  5.6352e-02, -7.7386e-02,  8.9354e-02,  1.4037e-01,\n",
      "         -3.3515e-02,  3.8260e-02, -3.7592e-02,  9.4399e-02,  5.4316e-02,\n",
      "         -1.4312e-02,  6.9107e-02,  7.1565e-02,  5.6494e-02,  7.2944e-02,\n",
      "         -1.0073e-01,  6.6054e-02, -1.3243e-01,  1.1429e-01, -3.5215e-02,\n",
      "          9.7127e-02,  9.9310e-02, -3.6200e-02, -2.0498e-02,  9.3283e-02,\n",
      "         -1.6823e-02,  3.4663e-02, -4.6424e-02,  5.8478e-02,  3.4547e-02,\n",
      "         -5.2421e-02,  2.0347e-03, -1.1436e-01, -1.0893e-01,  7.0235e-02,\n",
      "          2.2288e-01, -3.4948e-02, -1.3798e-01,  7.0055e-02,  1.7132e-01,\n",
      "          1.3305e-01,  3.0138e-01,  2.0090e-01, -9.6068e-02,  2.7854e-02,\n",
      "          1.9961e-01,  1.0367e-01,  1.0917e-01,  6.2917e-02,  1.6448e-01,\n",
      "          1.1987e-01, -6.6865e-02,  1.2141e-01, -1.4442e-01,  1.8113e-01,\n",
      "          1.9530e-01, -1.3963e-01,  1.4787e-01, -7.4109e-02, -2.6708e-02,\n",
      "          3.7491e-03,  7.8629e-02, -8.7739e-02, -1.7033e-02, -9.3251e-01,\n",
      "         -4.7354e-02,  1.9782e-01, -2.1438e-01,  1.0443e-01, -1.6851e-01,\n",
      "          2.2864e-01,  1.0677e-01,  6.5981e-02,  8.5374e-02,  2.3791e-02,\n",
      "         -2.6204e-01,  5.2758e-02, -1.4822e-02,  1.6207e-01,  2.1900e-01,\n",
      "         -3.5460e-01,  9.8926e-02,  3.3357e-02]])\n"
     ]
    }
   ],
   "source": [
    "# apply avg.pooling to word embeddings\n",
    "pooled_embeddings = last_hidden_state.detach().mean(dim=1)\n",
    "\n",
    "print(\"shape:\", pooled_embeddings.shape)\n",
    "print(\"\")\n",
    "print(pooled_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "stupid-lambda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-16T08:14:44.623617Z",
     "iopub.status.busy": "2021-05-16T08:14:44.616992Z",
     "iopub.status.idle": "2021-05-16T08:14:44.643759Z",
     "shell.execute_reply": "2021-05-16T08:14:44.643125Z"
    },
    "papermill": {
     "duration": 0.060522,
     "end_time": "2021-05-16T08:14:44.643964",
     "exception": false,
     "start_time": "2021-05-16T08:14:44.583442",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.020323</td>\n",
       "      <td>0.079437</td>\n",
       "      <td>-0.110324</td>\n",
       "      <td>-0.035171</td>\n",
       "      <td>0.226464</td>\n",
       "      <td>-0.027637</td>\n",
       "      <td>-0.077746</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.098366</td>\n",
       "      <td>-0.042963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085374</td>\n",
       "      <td>0.023791</td>\n",
       "      <td>-0.26204</td>\n",
       "      <td>0.052758</td>\n",
       "      <td>-0.014822</td>\n",
       "      <td>0.16207</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.354602</td>\n",
       "      <td>0.098926</td>\n",
       "      <td>0.033357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.020323  0.079437 -0.110324 -0.035171  0.226464 -0.027637 -0.077746   \n",
       "\n",
       "        7         8         9    ...       758       759      760       761  \\\n",
       "0 -0.000015  0.098366 -0.042963  ...  0.085374  0.023791 -0.26204  0.052758   \n",
       "\n",
       "        762      763    764       765       766       767  \n",
       "0 -0.014822  0.16207  0.219 -0.354602  0.098926  0.033357  \n",
       "\n",
       "[1 rows x 768 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pooled_embeddings.numpy()).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f4c84d64",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([1, 768])\n",
      "\n",
      "tensor([[ 2.0323e-02,  7.9437e-02, -1.1032e-01, -3.5171e-02,  2.2646e-01,\n",
      "         -2.7637e-02, -7.7746e-02, -1.4751e-05,  9.8366e-02, -4.2963e-02,\n",
      "         -5.9774e-02, -8.9828e-03, -1.1668e-01, -1.1539e-03, -5.1606e-02,\n",
      "          4.5606e-01,  3.0563e-01,  1.5051e-01,  3.0782e-02,  2.1871e-01,\n",
      "          5.1562e-02,  2.3083e-02,  4.0371e-01, -1.3139e-01,  1.3204e-01,\n",
      "         -8.1892e-02,  2.3483e-01,  6.0469e-02,  1.4729e-02, -1.1617e-01,\n",
      "          8.0583e-02,  6.4143e-02, -2.5171e-01, -2.0018e-01,  5.6423e-02,\n",
      "          3.3663e-02, -1.1430e-02,  1.8541e-02,  3.3345e-01, -9.3171e-02,\n",
      "         -1.1654e-01,  2.2614e-01,  1.1700e-01, -4.0023e-02,  1.1706e-01,\n",
      "         -6.9963e-02,  3.8883e-02,  4.5444e-01, -8.1482e-04,  1.7505e-01,\n",
      "          1.0154e-01, -1.3573e-01, -1.6017e-02,  3.5862e-03,  6.8536e-02,\n",
      "          1.3999e-01, -5.0224e-02, -1.3244e-01, -1.3408e-01,  5.0157e-03,\n",
      "          1.9436e-01, -1.8392e-01, -8.8699e-02, -2.2767e-02,  1.0509e-02,\n",
      "         -5.8541e-02,  3.8943e-02,  7.4841e-03, -1.8606e-01, -3.4022e-03,\n",
      "          1.2389e-01, -2.8656e-01,  3.2529e-02,  2.6665e-02,  9.9526e-02,\n",
      "         -2.3970e-01,  1.9252e-01, -4.7866e+00, -1.3475e-01,  2.4634e-02,\n",
      "         -5.2514e-05, -2.0905e-01, -1.8300e+00, -4.5689e-02, -3.0736e-02,\n",
      "          2.6734e-01,  5.3346e-02,  5.0606e-02,  3.5966e-02,  6.3563e-02,\n",
      "          4.3756e-02,  5.9892e-02,  6.0798e-02,  2.0692e-01, -1.5670e-01,\n",
      "          1.1381e-01,  8.2930e-02,  6.5995e-02, -1.2964e-01,  1.3056e-01,\n",
      "          1.0033e-02, -8.6466e-02,  1.8829e-01, -1.1813e-01, -3.1809e-02,\n",
      "         -1.5359e-01,  3.6622e-02, -1.4957e-02,  1.1845e-01, -3.6851e-02,\n",
      "         -5.9052e-02,  8.8161e-03, -1.2594e-01,  7.0197e-02, -1.4772e-01,\n",
      "          1.1466e-01,  8.2927e-03,  1.3873e-02, -1.4990e-01,  2.0128e-01,\n",
      "         -3.6731e-02, -4.9162e-02, -2.3457e-02, -3.0277e-02, -4.4135e-02,\n",
      "          1.4090e-01,  6.8588e-02, -1.5621e-01, -6.8993e-02,  1.3737e-01,\n",
      "         -1.4486e-03,  3.8049e-01,  4.9714e-02, -1.7769e-02,  6.4223e-02,\n",
      "          1.2659e-01,  1.3129e-01,  2.3188e-01, -3.8953e-02,  2.3731e-04,\n",
      "          1.1995e-02,  1.4815e-01,  1.2170e-01, -5.1344e-02, -4.6211e-02,\n",
      "         -3.3489e-02,  2.6715e-01,  1.0738e-02,  2.8133e-03, -1.2782e-01,\n",
      "         -5.7255e-02,  1.0956e-01, -5.6527e-03,  1.6130e-01, -1.4723e-01,\n",
      "         -5.6082e-01,  8.1826e-02,  2.5779e-01, -3.4652e-02, -8.9882e-02,\n",
      "          4.6978e-02, -1.0108e-01, -1.5031e-02,  1.3712e-01, -9.0742e-04,\n",
      "          6.9476e-02,  1.5742e-01,  5.4789e-02,  1.1121e-01, -2.1576e-01,\n",
      "          2.7354e-02, -6.2911e-02,  5.9046e-02,  8.2967e-02,  1.2202e-01,\n",
      "         -1.3924e-01, -1.0305e-01, -1.1636e-01, -1.2637e-02,  7.8453e-02,\n",
      "         -1.2819e-01,  1.9243e-01, -1.2168e-01, -5.7990e-02,  1.3665e-01,\n",
      "         -3.4386e-02, -2.0958e-01, -1.4565e-01,  7.5746e-02, -9.5652e-02,\n",
      "          1.0852e-01, -1.2087e-01, -4.6708e-02,  5.2989e-03, -1.4639e-02,\n",
      "          5.0346e-02,  4.8519e-02, -2.6666e-04,  2.4958e-02, -8.6895e-02,\n",
      "          4.7943e-03, -2.8139e-01, -1.6477e-01,  1.8729e-02,  2.8139e-01,\n",
      "         -6.2619e-02, -1.0307e-01,  3.1894e-02,  6.0858e-02, -1.8674e-01,\n",
      "          2.1108e-02,  2.2986e-01,  1.6393e-01, -1.7803e-01,  8.6865e-02,\n",
      "         -3.3508e-02,  1.2015e-01, -4.6650e-01,  6.0153e-02, -8.9162e-03,\n",
      "         -7.2374e-02,  1.3946e-01,  2.3412e-02,  2.2618e-01, -2.0241e-02,\n",
      "          8.5242e-02,  1.3986e-02,  2.1169e-01, -4.5457e-02, -2.3128e-02,\n",
      "         -2.5407e-02, -6.1046e-02,  5.1430e-02, -2.7377e-01,  2.1101e-01,\n",
      "         -1.4343e-01,  1.8793e-02,  3.3846e-02, -6.4712e-02,  8.5443e-02,\n",
      "          4.3706e-01, -5.0801e-04,  1.9264e-01,  7.7603e-02, -5.9127e-02,\n",
      "          1.3556e-01,  7.0578e-02,  2.9452e-01,  1.1862e-01,  1.7505e-01,\n",
      "         -1.0733e-01,  3.9632e-02,  1.0152e-01,  5.9892e-02, -9.2302e-03,\n",
      "          1.5065e-01, -1.5630e-02,  3.9491e-03,  1.5405e-01, -2.8058e-01,\n",
      "         -6.4269e-03,  6.8317e-02, -1.3800e-01, -1.2566e-01, -1.0110e-01,\n",
      "          7.3270e-03,  1.4929e-03,  3.3083e-02, -1.8013e-01,  6.7710e-02,\n",
      "         -3.6274e-02,  7.0049e-03, -5.9014e-02, -1.0125e-04,  3.4993e-02,\n",
      "          1.4482e-01,  7.2717e-02,  3.3734e-02, -3.5814e-01, -2.5542e-02,\n",
      "          5.9989e-02,  1.0053e-01, -1.6182e-01,  8.5669e-02,  7.8868e-02,\n",
      "         -2.9451e-01,  3.0925e-01, -7.6854e-02,  4.0078e-02, -1.1631e-01,\n",
      "          1.6863e-01, -3.1351e-02, -7.7942e-02,  1.6719e-01,  3.7881e-02,\n",
      "         -5.6542e-02, -6.1730e-02, -1.9533e-01,  9.0050e-02, -2.5571e-01,\n",
      "          2.2916e-01, -1.5668e-02, -9.6241e-02,  4.1238e-02,  2.1876e-01,\n",
      "          5.1811e-02,  3.1207e-02,  8.9781e-02,  1.3179e-01,  3.1986e-01,\n",
      "          3.8120e-02,  1.2678e-02, -8.3316e-02,  2.1065e-01, -1.7917e-01,\n",
      "         -8.3417e-02, -1.9308e-01,  2.8886e-02, -6.4374e-02,  2.4308e-01,\n",
      "         -4.2323e-02, -7.0847e-02, -1.2642e-02,  8.6043e-02, -2.4700e-01,\n",
      "         -6.7006e-03, -4.6293e-02,  5.7117e-02, -2.8503e-02,  1.6667e-01,\n",
      "         -2.7738e-01,  4.6867e-01, -1.8206e-02, -1.6270e-01,  1.6048e-02,\n",
      "          4.7872e-02, -1.0982e-02,  1.3070e-01, -3.0093e-02,  5.6475e-02,\n",
      "          1.2753e-01, -5.8623e-03,  3.3999e-02, -8.4873e-02,  1.0154e-01,\n",
      "          1.7953e-01,  3.0866e-01, -1.3697e-01,  1.7122e-01,  1.1414e-02,\n",
      "          9.9464e-02, -2.4940e-02,  9.2629e-02,  6.8630e-03, -9.0920e-03,\n",
      "          4.9076e-02,  1.4417e-01,  1.2753e-01, -1.6706e-02, -3.3098e-01,\n",
      "          2.2457e-01,  4.9540e-01,  1.0501e-01, -1.4938e-01,  2.1973e-01,\n",
      "         -2.2023e-01, -6.9804e-02,  1.9819e-01, -5.2257e-02,  7.2740e-02,\n",
      "          1.6290e-01,  3.6443e-02,  7.4377e-02, -2.8846e-01, -6.1780e-03,\n",
      "         -1.5330e-01, -9.1144e-02,  1.8058e-01,  1.0233e-01, -2.2608e-02,\n",
      "          1.1431e-01,  1.5401e-01, -1.2916e-01,  6.3905e-02, -1.4603e-01,\n",
      "         -9.9150e-02, -1.7629e-02, -1.1314e-02,  4.5495e-01,  3.4147e-02,\n",
      "         -8.5458e-02, -1.3733e-01,  4.5159e-02, -3.7373e-03,  8.2743e-02,\n",
      "          9.2124e-02,  5.1363e-02, -4.0661e-01, -1.6999e-01,  1.2903e-01,\n",
      "         -3.4384e-02,  2.7792e-02, -7.6106e-02, -9.1752e-02,  2.5522e-01,\n",
      "         -2.7681e-03, -1.5811e-01, -1.2817e-01, -8.7532e-02,  1.1320e-01,\n",
      "          1.5133e-01,  1.7614e-01,  1.2924e-02,  7.8648e-02,  2.8990e-03,\n",
      "          1.2495e-02, -1.2031e-02,  9.5160e-02,  9.1543e-02, -1.9894e-02,\n",
      "          1.1065e-02, -5.4143e-03, -5.7743e-02, -8.5999e-02, -2.7809e-01,\n",
      "          1.0077e-01, -3.7271e-03, -1.7822e-02, -3.2708e-02,  6.6092e-02,\n",
      "          1.1353e-02, -7.4027e-02,  1.1398e-01,  1.6263e-01, -1.2523e-01,\n",
      "         -1.7314e-01,  2.3436e-02, -1.8804e-01,  2.0134e-02,  1.0270e-01,\n",
      "          4.7693e-02,  1.6521e-01,  2.3024e-01, -1.0775e-03, -1.9410e-01,\n",
      "         -1.9047e-02, -1.4163e-02,  2.6257e-02,  6.6088e-02,  9.1651e-02,\n",
      "         -1.2099e-01,  1.6651e-01,  8.5531e-02, -2.8424e+00,  1.7480e-01,\n",
      "          9.3246e-02,  1.1525e-01,  8.1036e-02, -1.0714e-02, -7.5824e-02,\n",
      "          1.4083e-02,  2.2293e-01, -3.2821e-02,  9.9155e-02, -1.3665e-01,\n",
      "          2.1442e-03,  6.4307e-02,  2.3697e-01, -1.6986e-01,  1.0957e-01,\n",
      "          9.1785e-02, -3.6617e-02, -6.6747e-02, -1.8526e-02,  6.7679e-02,\n",
      "          2.1873e-02, -1.0448e-01,  1.1409e-01,  1.3055e-01, -1.1478e-01,\n",
      "          3.5816e-04,  6.3210e-02, -4.9694e-02,  2.1566e-02,  2.1471e-03,\n",
      "          1.2549e-01, -2.4227e-01,  1.1116e-01, -5.7732e-02,  2.1919e-01,\n",
      "         -1.3835e-01,  9.1048e-02, -4.4900e-02,  3.2044e-01,  2.1534e-01,\n",
      "          7.4880e-02, -1.2466e-01,  3.2365e-02, -2.5585e-01, -1.1541e-01,\n",
      "          2.3298e-01, -9.8598e-03, -1.8411e-01, -7.0962e-02,  9.6812e-02,\n",
      "          3.2171e-02,  1.6106e-01, -1.1148e-01,  2.4477e-01, -3.9449e-02,\n",
      "         -1.7794e-02,  5.3986e-01, -1.9859e-02,  3.3132e-02,  4.8063e-02,\n",
      "          4.4450e-03,  2.0593e-01,  3.1955e-02, -7.9910e-02, -9.4655e-02,\n",
      "         -3.2691e-02, -1.2679e-01, -1.0872e-01, -8.6466e-02,  1.1686e-01,\n",
      "         -1.8607e-01,  3.3247e-02,  2.2108e-01, -1.2121e-01, -7.0144e-02,\n",
      "         -2.1277e-02, -3.1511e-01, -3.9070e-02,  1.1912e-01,  1.3822e-01,\n",
      "         -2.2889e-01, -3.3229e-02,  8.1434e-02,  4.7820e-02, -2.6472e-01,\n",
      "         -5.7184e-02,  8.4748e-02, -2.1887e-02,  1.3561e-01,  6.2569e-02,\n",
      "         -9.6890e-02,  8.6944e-03, -4.9850e-03,  9.7571e-02,  3.7758e-02,\n",
      "          1.8031e-01, -3.3199e-02,  1.0422e-01,  4.8772e-02,  1.8295e-01,\n",
      "          1.4446e-01,  2.8198e-02,  1.2972e-01, -3.2357e-02,  1.9741e-01,\n",
      "          1.0368e-01, -2.0056e-01,  1.0220e-01,  3.4289e-02,  2.8554e-02,\n",
      "          6.7393e-02,  8.3224e-02, -2.6406e-01, -8.5564e-02,  1.5128e-01,\n",
      "          2.0610e-01, -1.2104e-01, -1.5316e-01,  2.1405e-01, -6.7119e-02,\n",
      "          2.7891e-02,  3.4936e-02,  6.8900e-02, -3.2132e-02,  2.1555e-01,\n",
      "         -5.0864e-02,  1.3855e-01, -4.6576e-02,  7.3315e-02,  3.6063e-02,\n",
      "         -1.1202e-01, -2.2411e-03,  1.5975e-01,  8.7913e+00,  2.7346e-02,\n",
      "          9.3223e-02,  1.6581e-01, -1.7718e-01, -3.0334e-02,  9.3729e-03,\n",
      "         -2.6308e-01,  1.0752e-01,  1.0599e-01, -1.0518e-01, -2.1183e-01,\n",
      "          5.6840e-04, -3.7821e-02, -4.2327e-03,  1.1295e-02, -2.6275e-01,\n",
      "         -4.9590e-02,  2.0656e-02,  1.7984e-02,  8.1446e-02,  1.3913e-02,\n",
      "          3.3798e-03, -2.6950e-01, -4.8762e-02, -1.0746e-01, -5.1818e-02,\n",
      "         -2.4345e-02, -9.5981e-02,  1.9496e-01,  5.2274e-02,  2.7274e-01,\n",
      "          5.1142e-02, -3.9024e-02,  2.3710e-02,  6.7889e-03, -6.2860e-02,\n",
      "         -4.1365e-02,  1.6583e-01,  1.4163e-01,  6.9964e-02,  8.6375e-02,\n",
      "          1.0757e-01,  1.3773e-01,  2.3397e-02,  3.1523e-02, -2.1126e-02,\n",
      "          5.9000e-02, -3.1480e-02, -1.9888e-01,  5.0350e-02,  3.0114e-02,\n",
      "          5.7217e-02, -7.3787e-02, -1.1659e-01,  9.2130e-02, -1.9929e-01,\n",
      "         -1.5437e-01, -2.2178e-02,  3.0989e-01,  1.5806e-01,  1.0021e-01,\n",
      "          5.6865e-02, -2.4838e-02, -3.9811e-01, -6.6272e-02, -7.6579e-02,\n",
      "         -7.1555e-02, -1.3273e-02,  6.8539e-02,  2.1343e-01, -1.2178e-01,\n",
      "         -1.8420e-01,  6.0164e-02, -1.2035e-01,  8.7767e-02,  7.0969e-01,\n",
      "         -1.3406e-01,  1.0615e-01,  1.4873e-01,  1.2052e-02,  2.0712e-01,\n",
      "         -3.2839e-02, -2.7878e-02, -9.5625e-02,  1.3958e-02,  1.3001e-01,\n",
      "         -6.8338e-02,  2.2321e-01, -1.3127e-01, -1.1877e-01, -9.2359e-02,\n",
      "          8.4717e-02,  2.3631e-02, -8.0206e-02, -2.1308e-02,  1.9737e-01,\n",
      "         -3.8582e-02,  5.6352e-02, -7.7386e-02,  8.9354e-02,  1.4037e-01,\n",
      "         -3.3515e-02,  3.8260e-02, -3.7592e-02,  9.4399e-02,  5.4316e-02,\n",
      "         -1.4312e-02,  6.9107e-02,  7.1565e-02,  5.6494e-02,  7.2944e-02,\n",
      "         -1.0073e-01,  6.6054e-02, -1.3243e-01,  1.1429e-01, -3.5215e-02,\n",
      "          9.7127e-02,  9.9310e-02, -3.6200e-02, -2.0498e-02,  9.3283e-02,\n",
      "         -1.6823e-02,  3.4663e-02, -4.6424e-02,  5.8478e-02,  3.4547e-02,\n",
      "         -5.2421e-02,  2.0347e-03, -1.1436e-01, -1.0893e-01,  7.0235e-02,\n",
      "          2.2288e-01, -3.4948e-02, -1.3798e-01,  7.0055e-02,  1.7132e-01,\n",
      "          1.3305e-01,  3.0138e-01,  2.0090e-01, -9.6068e-02,  2.7854e-02,\n",
      "          1.9961e-01,  1.0367e-01,  1.0917e-01,  6.2917e-02,  1.6448e-01,\n",
      "          1.1987e-01, -6.6865e-02,  1.2141e-01, -1.4442e-01,  1.8113e-01,\n",
      "          1.9530e-01, -1.3963e-01,  1.4787e-01, -7.4109e-02, -2.6708e-02,\n",
      "          3.7491e-03,  7.8629e-02, -8.7739e-02, -1.7033e-02, -9.3251e-01,\n",
      "         -4.7354e-02,  1.9782e-01, -2.1438e-01,  1.0443e-01, -1.6851e-01,\n",
      "          2.2864e-01,  1.0677e-01,  6.5981e-02,  8.5374e-02,  2.3791e-02,\n",
      "         -2.6204e-01,  5.2758e-02, -1.4822e-02,  1.6207e-01,  2.1900e-01,\n",
      "         -3.5460e-01,  9.8926e-02,  3.3357e-02]])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.020323</td>\n",
       "      <td>0.079437</td>\n",
       "      <td>-0.110324</td>\n",
       "      <td>-0.035171</td>\n",
       "      <td>0.226464</td>\n",
       "      <td>-0.027637</td>\n",
       "      <td>-0.077746</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.098366</td>\n",
       "      <td>-0.042963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085374</td>\n",
       "      <td>0.023791</td>\n",
       "      <td>-0.26204</td>\n",
       "      <td>0.052758</td>\n",
       "      <td>-0.014822</td>\n",
       "      <td>0.16207</td>\n",
       "      <td>0.219</td>\n",
       "      <td>-0.354602</td>\n",
       "      <td>0.098926</td>\n",
       "      <td>0.033357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.020323  0.079437 -0.110324 -0.035171  0.226464 -0.027637 -0.077746   \n",
       "\n",
       "        7         8         9    ...       758       759      760       761  \\\n",
       "0 -0.000015  0.098366 -0.042963  ...  0.085374  0.023791 -0.26204  0.052758   \n",
       "\n",
       "        762      763    764       765       766       767  \n",
       "0 -0.014822  0.16207  0.219 -0.354602  0.098926  0.033357  \n",
       "\n",
       "[1 rows x 768 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_state.shape\n",
    "\n",
    "# apply avg.pooling to word embeddings\n",
    "pooled_embeddings = last_hidden_state.detach().mean(dim=1)\n",
    "\n",
    "print(\"shape:\", pooled_embeddings.shape)\n",
    "print(\"\")\n",
    "print(pooled_embeddings)\n",
    "\n",
    "pd.DataFrame(pooled_embeddings.numpy()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-florist",
   "metadata": {
    "papermill": {
     "duration": 0.027059,
     "end_time": "2021-05-16T08:14:44.698362",
     "exception": false,
     "start_time": "2021-05-16T08:14:44.671303",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "note!: pooler output \"not\" equal pooled_embeddings we calculated  \n",
    "What is pooler output ?  \n",
    "-> It takes the representation from the [CLS] token from top layer of RoBERTa encoder, and feed that through another dense layer.  \n",
    "reference: https://github.com/google-research/bert/blob/cc7051dc592802f501e8a6f71f8fb3cf9de95dc9/modeling.py#L224-L232  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e90649",
   "metadata": {},
   "source": [
    "# Loading COCO  - (Work in Progress)\n",
    "- Two of the best tools for this are the official COCO APIs and FiftyOne.\n",
    "- Official COCO APIs provide basic functionality to load and compute dataset-wide evaluation on your dataset.\n",
    "- FiftyOne is recommended as it provides similar functionality to the cocoapi, along with a powerful API and GUI designed specifically to make it as easy as possible for you to explore, analyze, and work with your data.\n",
    "\n",
    "Importing labeled datasets in formats such as COCO, you may find it more natural to provide the data_path and labels_path parameters to independently specify the location of the source media on disk and the annotations file containing the labels to import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6732f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_anotation_PATH=\"D:/Documents/TUM/WS2022/XAI/MS_COCO/annotations_trainval2017/annotations/captions_val2017.json\"\n",
    "COCO_data_PATH=\"D:/Documents/TUM/WS2022/XAI/MS_COCO/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61bd1b68",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycocotools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14400\\1709577621.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpycocotools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoco\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCOCO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mskimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pycocotools'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "\n",
    "# dataDir='..'\n",
    "# dataType='val2017'\n",
    "# annFile='{}/annotations/instances_{}.json'.format(dataDir,dataType)\n",
    "\n",
    "\n",
    "coco_caps=COCO(COCO_anotation_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44475b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display COCO categories and supercategories\n",
    "cats = coco.loadCats(coco.getCatIds())\n",
    "nms=[cat['name'] for cat in cats]\n",
    "print('COCO categories: \\n{}\\n'.format(' '.join(nms)))\n",
    "\n",
    "nms = set([cat['supercategory'] for cat in cats])\n",
    "print('COCO supercategories: \\n{}'.format(' '.join(nms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54792baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and display caption annotations\n",
    "annIds = coco_caps.getAnnIds(imgIds=img['id']);\n",
    "anns = coco_caps.loadAnns(annIds)\n",
    "coco_caps.showAnns(anns)\n",
    "plt.imshow(I); plt.axis('off'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0680b9a7",
   "metadata": {},
   "source": [
    "above is COCO apis\n",
    "\n",
    "below is fiftyone apis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7348adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.coco as fouc\n",
    "\n",
    "\n",
    "dataset_annotation = fouc.load_coco_detection_annotations(COCO_anotation_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ad804c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View summary info about the dataset\n",
    "print(dataset_annotation)\n",
    "\n",
    "# Print the first few samples in the dataset\n",
    "print(dataset_annotation.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fcadefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 0/0 [8.0ms elapsed, ? remaining, ? samples/s]   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:eta.core.utils: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 0/0 [8.0ms elapsed, ? remaining, ? samples/s]   \n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "'''\n",
    "check https://voxel51.com/docs/fiftyone/api/fiftyone.utils.coco.html#fiftyone.utils.coco.COCODetectionDatasetImporter\n",
    "for COCO import options.\n",
    "Load via FiftyOne\n",
    "'''\n",
    "# Import the dataset\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_type=fo.types.COCODetectionDataset,    #COCO Dataset\n",
    "    labels_path=COCO_Anotation_PATH,   #annotations path\n",
    "    #data_path=data_path, #images path \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bae914a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:        2022.11.24.19.00.37\n",
      "Media type:  None\n",
      "Num samples: 0\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:       fiftyone.core.fields.ObjectIdField\n",
      "    filepath: fiftyone.core.fields.StringField\n",
      "    tags:     fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.Metadata)\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "'''The primary way of interacting with your dataset is through views. \n",
    "Every query you make will give you a different view into your dataset, like sorting by samples with the most number of objects'''\n",
    "# View summary info about the dataset\n",
    "print(dataset)\n",
    "\n",
    "# Print the first few samples in the dataset\n",
    "print(dataset.head())\n",
    "\n",
    "#To visualize youra dataset, launch the FiftyOne App:\n",
    "# session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b062e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "train_dataset = TrainValidDataset(df_train, tokenizer, Settings.max_len)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=Settings.batch_size,\n",
    "                          shuffle=True, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc47f1a6",
   "metadata": {},
   "source": [
    "## Inference on COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-charm",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-16T08:14:28.140415Z",
     "iopub.status.busy": "2021-05-16T08:14:28.139644Z",
     "iopub.status.idle": "2021-05-16T08:14:28.695919Z",
     "shell.execute_reply": "2021-05-16T08:14:28.696470Z"
    },
    "papermill": {
     "duration": 0.585971,
     "end_time": "2021-05-16T08:14:28.696678",
     "exception": false,
     "start_time": "2021-05-16T08:14:28.110707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make mini batch data\n",
    "batch = next(iter(train_loader))\n",
    "ids = batch[\"ids\"].to(Settings.device)\n",
    "mask = batch[\"mask\"].to(Settings.device)\n",
    "targets = batch[\"targets\"].to(Settings.device) #todo: what is targets? is it token type ids?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90c6d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # inference on input\n",
    "# output = model(input_ids=ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "output = model(ids, mask)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
